{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Attention Rate of Patients by Monitoring their Emotions\n",
    "\n",
    "- Name: Akshara Shukla\n",
    "- Class: AI43\n",
    "- Genuine Challenge_01\n",
    "\n",
    "## Introduction\n",
    "This project consists of creating a report on aiding doctors in large multinational hospitals to \n",
    "understand different patient behaviors. Fundamentally, the project goal is to showcase the \n",
    "power of Artificial Intelligence and Machine Learning in healthcare and how to provide a \n",
    "better patient experience by classifying them on their emotions, mainly the four basic \n",
    "emotions (happy, sad, angry, and relaxing) and accompanying the ones that are in pain and \n",
    "require immediate or more attention and care. The system is Areya and it aims at providing some portrait of notification to the doctors about patients who illustrate negative emotions i.e., angry and sad.\n",
    "\n",
    "This notebook consists the testing of the dataset FER-2013 in real time by using the default camera attached in my pc. The algorithm chosen was building a convolutional neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We start with importing needed/required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model  #loading the saved model from pc\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image  \n",
    "import cv2 #for conducting computer vision\n",
    "import numpy as np #for rescaling and normalising face pixels detected from camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start our testing by first loading a <b>haarcascade_frontalface_default.xml</b>. This is an xml file which is provided by cv2 library for detecting face in any video frame. This file was taken from the github repository of cv2.\n",
    "Following, loading our model from the path defined on computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file to detect a face in our video frame\n",
    "face_classifier = cv2.CascadeClassifier(r'C:\\Users\\PC\\Evernote\\Logs\\emotion_recognition\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "#loading the model\n",
    "model =load_model(r'C:\\Users\\PC\\Evernote\\Logs\\emotion_recognition\\my_h5_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicitly specifying the different emotions which will be exhibited by patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Angry','Happy','Relaxed','Sad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the camera turned on, using the .VideoCapture() function provided by cv2 library and setting the paramter to 0, since I want to turn my default camera from my pc on and assigning it to the variable cam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the detailed steps which will be carried while the camera is turned on. Following will be the steps carried out in brief: \n",
    "- It will essentially start by first detecting if a face is presented in the camera frame which is carried out with the help of cam.read() function. Additionally, it will be saving the face and forming a rectangle around it which will have our region of interest i.e. the facial expression of an individual or the patients in our project. \n",
    "- After detecting the region of interest, it will be normalizing the pixel values to our default image dimensions of our dataframe, which is 48x48. And predicting the emotions of our patients with their attention rate.\n",
    "- When emotions displayed aren't Happy and Relaxed, a message of \"Needs Immediate Attention\" would pop up.\n",
    "- In case of no face getting detected, the model would return a 'No face detected' message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cam.read() #read the face from the camera/frame\n",
    "    #labels = []\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #convert it into grayscale(train data = grayscale)\n",
    "    #putting neighbors as 5 which is similar to the hyperparamter of knn, where k = 5\n",
    "    #so if our model is getting confused in detecting the emotion, it can understand the 5 nearest labels and then further predict\n",
    "    faces = face_classifier.detectMultiScale(gray,1.3,5) #scale down our frame/input img because in the webcam, the pixel values will be very high\n",
    "\n",
    "    #draw a rectange around our face\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        #in our region of interest, we want to convert it into gray to take it as an input\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        #resizing it to the original size of our img\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "    # rect,face,image = face_detector(frame)\n",
    "\n",
    "        #when there is a face detected\n",
    "        if np.sum([roi_gray])!=0:\n",
    "            roi = roi_gray.astype('float')/255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis = 0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "            preds = model.predict(roi)[0]\n",
    "            label=emotions[preds.argmax()]\n",
    "            label_position = (x,y)\n",
    "            #using cv2.font_hershey_simplex to diplay the text on the camera with color green and size of 3.\n",
    "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "            #Inserting the conditions to display the message when emotions aren't Happy and Relaxed on the camera\n",
    "            if(label != 'Happy' and label != 'Relaxed'):\n",
    "                cv2.putText(frame,'Needs Immediate Attention',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "            else:\n",
    "                cv2.putText(frame,'',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        else:\n",
    "            cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "    cv2.imshow('Emotion Detector',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below chunk helps to close the window which opens when the above chunk is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, the haarcasade file from the cv2 library worked perfectly in detecting if a face is present in the frame or not. It didn't get distracted by any light dimness or excessive brightness. Our model did a great job in detecting emotions mainly Happy, as seen from the confusion matrix, all the emotions were merely classified as Happy. Having said this, the goal for future implementation is to improve the performance in detecting other emotions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
